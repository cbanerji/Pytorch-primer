{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised learning using a feedforward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " #Features = 2\n",
      "\n",
      " #Samples = 63\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number_of_claims</th>\n",
       "      <th>Payment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>108</td>\n",
       "      <td>392.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>46.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>15.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>124</td>\n",
       "      <td>422.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>119.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>9</td>\n",
       "      <td>87.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>31</td>\n",
       "      <td>209.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>14</td>\n",
       "      <td>95.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>53</td>\n",
       "      <td>244.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>26</td>\n",
       "      <td>187.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Number_of_claims  Payment \n",
       "0                108     392.5\n",
       "1                 19      46.2\n",
       "2                 13      15.7\n",
       "3                124     422.2\n",
       "4                 40     119.4\n",
       "..               ...       ...\n",
       "57                 9      87.4\n",
       "58                31     209.8\n",
       "59                14      95.5\n",
       "60                53     244.6\n",
       "61                26     187.5\n",
       "\n",
       "[62 rows x 2 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get and preprocess the data according to our requirement\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('insurance.csv')\n",
    "print('\\n #Features = '+str(data.shape[1]))\n",
    "print('\\n #Samples = '+str(data.shape[0]))\n",
    "indx = data.index[data['Number_of_claims'] == 0].tolist()\n",
    "data.drop(indx).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following is the code for the training network.\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "torch.manual_seed(23)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "dat = data.to_numpy(dtype= np.float32) # dataframe  to numpy \n",
    "# Split data into input and labels\n",
    "x = torch.from_numpy(dat[:,[0]])\n",
    "x = torch.nn.functional.normalize(x, p=2.0, dim = 0)\n",
    "y = torch.from_numpy(dat[:,[1]])\n",
    "\n",
    "# Split data into test and train\n",
    "train_size = int(0.8*len(dat))\n",
    "test_size = len(dat)- train_size\n",
    "x_train, x_test = torch.utils.data.random_split(x, [train_size, test_size])\n",
    "y_train, y_test = torch.utils.data.random_split(y, [train_size, test_size])\n",
    "\n",
    "# define train and test Datasets\n",
    "class CustomDataset_train(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.num_samples = len(x_train)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_train[index], self.y_train[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "class CustomDataset_test(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.num_samples = len(x_test)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_test[index], self.y_test[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "data_train = CustomDataset_train()\n",
    "data_test = CustomDataset_test()\n",
    "\n",
    "batch_size= 1 #set batch size\n",
    "\n",
    "train_dataloader = DataLoader(dataset = data_train, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset = data_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myModel(\n",
      "  (fc1): Linear(in_features=1, out_features=128, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "input_size = x.shape[1]\n",
    "output_size = y.shape[1]\n",
    "\n",
    "# define network architecture\n",
    "class myModel(nn.Module):\n",
    "    def __init__(self,input_dim, output_dim, hidden_size1 =128, hidden_size2 = 128):\n",
    "        super(myModel, self).__init__()\n",
    "        #define network layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "      \n",
    "    def forward(self,x):\n",
    "        \n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "model = myModel(input_size, output_size)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function, optimizer and hyperparameters\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "total_step = len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "overall_train_loss: 578157.4938972294\n",
      "overall_Test_loss: 497389.502045 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "overall_train_loss: 573816.5605957508\n",
      "overall_Test_loss: 495449.749741 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "overall_train_loss: 568904.4708471298\n",
      "overall_Test_loss: 493223.868774 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "overall_train_loss: 563359.6154277325\n",
      "overall_Test_loss: 490469.767403 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "overall_train_loss: 556669.979414463\n",
      "overall_Test_loss: 487566.074448 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "overall_train_loss: 549203.3427761495\n",
      "overall_Test_loss: 483894.458115 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "overall_train_loss: 540310.9234151542\n",
      "overall_Test_loss: 479875.219219 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "overall_train_loss: 530552.5084577501\n",
      "overall_Test_loss: 475138.053507 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "overall_train_loss: 519643.1939769387\n",
      "overall_Test_loss: 470063.786004 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "overall_train_loss: 507723.57725811005\n",
      "overall_Test_loss: 464701.890615 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "overall_train_loss: 495024.1150479317\n",
      "overall_Test_loss: 458732.791533 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "overall_train_loss: 481701.65078147734\n",
      "overall_Test_loss: 452199.201950 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "overall_train_loss: 467223.07164341956\n",
      "overall_Test_loss: 445948.776800 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "overall_train_loss: 452526.0282741748\n",
      "overall_Test_loss: 439104.021884 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "overall_train_loss: 437383.24618673325\n",
      "overall_Test_loss: 432036.333744 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "overall_train_loss: 422626.4331892729\n",
      "overall_Test_loss: 424029.010136 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "overall_train_loss: 406916.35254228115\n",
      "overall_Test_loss: 416528.082180 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "overall_train_loss: 391510.4371585846\n",
      "overall_Test_loss: 409297.673355 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "overall_train_loss: 376452.5314900875\n",
      "overall_Test_loss: 401918.781448 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "overall_train_loss: 361683.5428584814\n",
      "overall_Test_loss: 394386.297050 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "overall_train_loss: 347343.38670258597\n",
      "overall_Test_loss: 386952.223694 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "overall_train_loss: 333713.3404957056\n",
      "overall_Test_loss: 379498.566206 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "overall_train_loss: 320531.11768488586\n",
      "overall_Test_loss: 372777.296021 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "overall_train_loss: 308486.81266635656\n",
      "overall_Test_loss: 365500.997356 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "overall_train_loss: 297157.4881718159\n",
      "overall_Test_loss: 358856.454445 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "overall_train_loss: 286345.3098217733\n",
      "overall_Test_loss: 352989.575857 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "overall_train_loss: 276591.75259274244\n",
      "overall_Test_loss: 347027.502857 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "overall_train_loss: 267879.9928586483\n",
      "overall_Test_loss: 341934.717624 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "overall_train_loss: 259995.51362961996\n",
      "overall_Test_loss: 336826.637059 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "overall_train_loss: 252553.95166352764\n",
      "overall_Test_loss: 331364.437691 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "overall_train_loss: 246291.71474123\n",
      "overall_Test_loss: 326136.264061 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "overall_train_loss: 240501.4769025743\n",
      "overall_Test_loss: 321559.271812 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "overall_train_loss: 235160.4221625328\n",
      "overall_Test_loss: 318403.672989 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "overall_train_loss: 230740.489256382\n",
      "overall_Test_loss: 314838.000504 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "overall_train_loss: 227332.53216409683\n",
      "overall_Test_loss: 311073.508990 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "overall_train_loss: 223840.99382066727\n",
      "overall_Test_loss: 307923.836717 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "overall_train_loss: 221016.1464395523\n",
      "overall_Test_loss: 305064.670505 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "overall_train_loss: 218345.76601314545\n",
      "overall_Test_loss: 303465.272611 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "overall_train_loss: 216187.15622520447\n",
      "overall_Test_loss: 300845.261095 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "overall_train_loss: 214494.74388504028\n",
      "overall_Test_loss: 298109.703128 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "overall_train_loss: 212988.97349739075\n",
      "overall_Test_loss: 296368.934072 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "overall_train_loss: 211412.1371254921\n",
      "overall_Test_loss: 294813.085474 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "overall_train_loss: 210268.59951257706\n",
      "overall_Test_loss: 293482.319685 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "overall_train_loss: 209281.7844028473\n",
      "overall_Test_loss: 291787.355157 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "overall_train_loss: 208608.73031527695\n",
      "overall_Test_loss: 290115.406520 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "overall_train_loss: 208141.74867234612\n",
      "overall_Test_loss: 288535.132273 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "overall_train_loss: 207137.7999680359\n",
      "overall_Test_loss: 287707.892282 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "overall_train_loss: 207010.99580849707\n",
      "overall_Test_loss: 286881.045219 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "overall_train_loss: 206451.12101387978\n",
      "overall_Test_loss: 285737.911543 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "overall_train_loss: 206184.3756158352\n",
      "overall_Test_loss: 285659.305183 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "overall_train_loss: 205749.36476933956\n",
      "overall_Test_loss: 284604.561291 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "overall_train_loss: 205643.6477088614\n",
      "overall_Test_loss: 283922.010303 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "overall_train_loss: 205251.93805712834\n",
      "overall_Test_loss: 283185.007893 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "overall_train_loss: 205480.9725701809\n",
      "overall_Test_loss: 282419.079716 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "overall_train_loss: 204983.5371658504\n",
      "overall_Test_loss: 282266.263798 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "overall_train_loss: 204915.85573863983\n",
      "overall_Test_loss: 281753.445488 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "overall_train_loss: 204668.02522206306\n",
      "overall_Test_loss: 281239.034691 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "overall_train_loss: 204643.49791193008\n",
      "overall_Test_loss: 281183.086651 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "overall_train_loss: 204510.10591697693\n",
      "overall_Test_loss: 280602.196533 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "overall_train_loss: 204491.2883901596\n",
      "overall_Test_loss: 280029.380699 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "overall_train_loss: 204491.72493553162\n",
      "overall_Test_loss: 280088.048855 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "overall_train_loss: 204566.32702875137\n",
      "overall_Test_loss: 279801.437744 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "overall_train_loss: 204450.51391029358\n",
      "overall_Test_loss: 279238.651634 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "overall_train_loss: 204272.005569458\n",
      "overall_Test_loss: 278909.817844 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "overall_train_loss: 204410.94382333755\n",
      "overall_Test_loss: 279139.554977 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "overall_train_loss: 204361.52038145065\n",
      "overall_Test_loss: 279018.712370 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "overall_train_loss: 204351.36889123917\n",
      "overall_Test_loss: 278704.677650 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "overall_train_loss: 204204.67835617065\n",
      "overall_Test_loss: 278616.273464 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "overall_train_loss: 204306.62938594818\n",
      "overall_Test_loss: 278192.352320 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "overall_train_loss: 204239.3452692032\n",
      "overall_Test_loss: 278327.945347 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "overall_train_loss: 204211.69773340225\n",
      "overall_Test_loss: 278531.498318 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "overall_train_loss: 204190.45859384537\n",
      "overall_Test_loss: 277956.379047 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall_train_loss: 204108.07808327675\n",
      "overall_Test_loss: 277656.566315 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "overall_train_loss: 204204.99337792397\n",
      "overall_Test_loss: 277826.785038 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "overall_train_loss: 204431.06451249123\n",
      "overall_Test_loss: 277779.650936 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "overall_train_loss: 204151.29968500137\n",
      "overall_Test_loss: 277471.751390 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "overall_train_loss: 204282.6989045143\n",
      "overall_Test_loss: 277604.925447 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "overall_train_loss: 204149.73036766052\n",
      "overall_Test_loss: 277913.421257 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "overall_train_loss: 204098.06415104866\n",
      "overall_Test_loss: 277520.211874 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "overall_train_loss: 204654.11032950878\n",
      "overall_Test_loss: 276951.468267 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "overall_train_loss: 204316.13097953796\n",
      "overall_Test_loss: 277773.277857 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "overall_train_loss: 204038.83540987968\n",
      "overall_Test_loss: 277599.250702 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "overall_train_loss: 204454.1893876493\n",
      "overall_Test_loss: 276847.378424 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "overall_train_loss: 204108.48689079285\n",
      "overall_Test_loss: 277371.011614 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "overall_train_loss: 204154.4354763627\n",
      "overall_Test_loss: 277015.544834 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "overall_train_loss: 204069.7130765915\n",
      "overall_Test_loss: 277004.744110 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "overall_train_loss: 204351.02372778952\n",
      "overall_Test_loss: 277014.275578 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "overall_train_loss: 204066.97949278355\n",
      "overall_Test_loss: 277319.685894 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "overall_train_loss: 204018.3527789116\n",
      "overall_Test_loss: 277104.790895 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "overall_train_loss: 204127.36323857307\n",
      "overall_Test_loss: 277204.049004 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "overall_train_loss: 204150.72759628296\n",
      "overall_Test_loss: 277391.737677 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "overall_train_loss: 204107.97403037548\n",
      "overall_Test_loss: 277404.660675 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "overall_train_loss: 204220.4340312183\n",
      "overall_Test_loss: 276642.452876 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "overall_train_loss: 204035.96707946062\n",
      "overall_Test_loss: 277136.666904 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "overall_train_loss: 204018.13569426537\n",
      "overall_Test_loss: 277305.437170 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "overall_train_loss: 204145.36249256134\n",
      "overall_Test_loss: 276834.057858 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "overall_train_loss: 203938.85065865517\n",
      "overall_Test_loss: 277324.059759 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "overall_train_loss: 204154.34083276987\n",
      "overall_Test_loss: 276954.830965 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "overall_train_loss: 203928.12322425842\n",
      "overall_Test_loss: 277180.298082 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "overall_train_loss: 204058.86461532116\n",
      "overall_Test_loss: 277200.700573 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "overall_train_loss: 204022.5366165638\n",
      "overall_Test_loss: 277118.559095 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "overall_train_loss: 203983.35411822796\n",
      "overall_Test_loss: 277049.387754 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "overall_train_loss: 203962.39992439747\n",
      "overall_Test_loss: 277216.073599 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "overall_train_loss: 204185.917943798\n",
      "overall_Test_loss: 276827.058842 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "overall_train_loss: 203916.55634570122\n",
      "overall_Test_loss: 277048.180115 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "overall_train_loss: 203917.1462827921\n",
      "overall_Test_loss: 277007.183701 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "overall_train_loss: 204011.14560440183\n",
      "overall_Test_loss: 277116.938526 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "overall_train_loss: 204171.06157293916\n",
      "overall_Test_loss: 276770.137579 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "overall_train_loss: 204169.25391209126\n",
      "overall_Test_loss: 276564.433908 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "overall_train_loss: 203903.85059705377\n",
      "overall_Test_loss: 277115.739225 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "overall_train_loss: 204191.20210613124\n",
      "overall_Test_loss: 276843.485477 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "overall_train_loss: 204068.66358673573\n",
      "overall_Test_loss: 277211.808413 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "overall_train_loss: 204009.26896959543\n",
      "overall_Test_loss: 277285.496323 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "overall_train_loss: 204078.70216071606\n",
      "overall_Test_loss: 277442.278948 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "overall_train_loss: 204191.4533714056\n",
      "overall_Test_loss: 276774.280970 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "overall_train_loss: 203888.34503267705\n",
      "overall_Test_loss: 277021.114056 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "overall_train_loss: 203912.1368883252\n",
      "overall_Test_loss: 277285.604961 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "overall_train_loss: 204026.9079580903\n",
      "overall_Test_loss: 277048.523027 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "overall_train_loss: 203941.97582632303\n",
      "overall_Test_loss: 277306.110119 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "overall_train_loss: 204148.89972507954\n",
      "overall_Test_loss: 277102.884869 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "overall_train_loss: 203911.93840679526\n",
      "overall_Test_loss: 276779.633864 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "overall_train_loss: 203960.1778896153\n",
      "overall_Test_loss: 276741.172386 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "overall_train_loss: 203920.25178197026\n",
      "overall_Test_loss: 277020.064552 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "overall_train_loss: 204287.09939991892\n",
      "overall_Test_loss: 276977.504017 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "overall_train_loss: 204003.97716158628\n",
      "overall_Test_loss: 276902.318552 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "overall_train_loss: 204116.15978610516\n",
      "overall_Test_loss: 277378.368464 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "overall_train_loss: 203909.8397360444\n",
      "overall_Test_loss: 276908.974852 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "overall_train_loss: 203841.7608411163\n",
      "overall_Test_loss: 276895.622564 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "overall_train_loss: 204151.792557925\n",
      "overall_Test_loss: 276754.531197 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "overall_train_loss: 203817.4787323773\n",
      "overall_Test_loss: 277217.083450 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "overall_train_loss: 203934.38041314483\n",
      "overall_Test_loss: 277127.145079 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "overall_train_loss: 203869.1329139471\n",
      "overall_Test_loss: 277313.091682 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "overall_train_loss: 203859.2120435238\n",
      "overall_Test_loss: 277049.917370 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "overall_train_loss: 203971.6441089511\n",
      "overall_Test_loss: 277013.320436 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "overall_train_loss: 203892.1942936778\n",
      "overall_Test_loss: 276925.571115 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "overall_train_loss: 203860.85533377528\n",
      "overall_Test_loss: 277307.409575 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "overall_train_loss: 203785.52756369114\n",
      "overall_Test_loss: 277287.670202 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "overall_train_loss: 203959.68092912436\n",
      "overall_Test_loss: 277273.011299 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "overall_train_loss: 203902.21186333895\n",
      "overall_Test_loss: 277440.664728 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "overall_train_loss: 204002.30568099022\n",
      "overall_Test_loss: 276840.322937 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "overall_train_loss: 203826.66060049087\n",
      "overall_Test_loss: 277088.960224 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "overall_train_loss: 203992.8478833437\n",
      "overall_Test_loss: 277227.763567 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "overall_train_loss: 203899.15746232867\n",
      "overall_Test_loss: 277344.865461 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "overall_train_loss: 203788.43653726578\n",
      "overall_Test_loss: 277368.635052 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "overall_train_loss: 203937.84944461286\n",
      "overall_Test_loss: 277176.554348 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "overall_train_loss: 203901.90876906365\n",
      "overall_Test_loss: 277348.954790 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall_train_loss: 203888.88887169957\n",
      "overall_Test_loss: 277308.106419 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "overall_train_loss: 203882.85822818056\n",
      "overall_Test_loss: 276909.737078 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "overall_train_loss: 203946.08521813154\n",
      "overall_Test_loss: 277310.699268 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "overall_train_loss: 203762.8485864997\n",
      "overall_Test_loss: 276984.237799 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "overall_train_loss: 204071.49230638146\n",
      "overall_Test_loss: 277026.710140 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "overall_train_loss: 203844.75595141947\n",
      "overall_Test_loss: 276844.730230 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "overall_train_loss: 203936.76785098016\n",
      "overall_Test_loss: 277134.250092 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "overall_train_loss: 203768.90557417274\n",
      "overall_Test_loss: 277112.054338 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "overall_train_loss: 204085.18791867048\n",
      "overall_Test_loss: 276889.790344 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "overall_train_loss: 203794.26780713908\n",
      "overall_Test_loss: 276866.128780 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "overall_train_loss: 203923.43145515397\n",
      "overall_Test_loss: 276864.756615 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "overall_train_loss: 203921.0784788737\n",
      "overall_Test_loss: 277476.516376 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "overall_train_loss: 204056.38147877716\n",
      "overall_Test_loss: 276823.489115 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "overall_train_loss: 203871.6459961105\n",
      "overall_Test_loss: 276913.756706 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "overall_train_loss: 204091.92569981515\n",
      "overall_Test_loss: 276902.959078 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "overall_train_loss: 203829.8427638486\n",
      "overall_Test_loss: 277363.584793 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "overall_train_loss: 203795.3240628168\n",
      "overall_Test_loss: 277037.358412 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "overall_train_loss: 203865.29082256556\n",
      "overall_Test_loss: 277486.326099 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "overall_train_loss: 203766.04645180702\n",
      "overall_Test_loss: 277604.141716 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "overall_train_loss: 204154.59627443552\n",
      "overall_Test_loss: 276825.822995 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "overall_train_loss: 203875.07071714482\n",
      "overall_Test_loss: 277081.285727 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "overall_train_loss: 203716.0658122003\n",
      "overall_Test_loss: 277248.241426 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "overall_train_loss: 203778.38458081707\n",
      "overall_Test_loss: 276997.348789 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "overall_train_loss: 203918.63198074698\n",
      "overall_Test_loss: 277272.702715 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "overall_train_loss: 203707.96469598264\n",
      "overall_Test_loss: 277130.774967 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "overall_train_loss: 203948.22194431722\n",
      "overall_Test_loss: 276702.882782 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "overall_train_loss: 203839.75375660136\n",
      "overall_Test_loss: 277318.919802 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "overall_train_loss: 203885.7048983276\n",
      "overall_Test_loss: 277193.709808 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "overall_train_loss: 203902.2221236825\n",
      "overall_Test_loss: 276937.363440 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "overall_train_loss: 203738.75345434248\n",
      "overall_Test_loss: 277375.017250 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "overall_train_loss: 203746.24117537588\n",
      "overall_Test_loss: 277334.559130 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "overall_train_loss: 203917.449896425\n",
      "overall_Test_loss: 277468.498943 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "overall_train_loss: 203881.39676718414\n",
      "overall_Test_loss: 277146.713480 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "overall_train_loss: 203727.24511052296\n",
      "overall_Test_loss: 277181.688375 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "overall_train_loss: 203732.99843276665\n",
      "overall_Test_loss: 277437.481726 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "overall_train_loss: 203713.9800168611\n",
      "overall_Test_loss: 277123.864628 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "overall_train_loss: 203988.32000917196\n",
      "overall_Test_loss: 277719.709612 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "overall_train_loss: 203731.63900576957\n",
      "overall_Test_loss: 277194.642433 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "overall_train_loss: 203722.14365700632\n",
      "overall_Test_loss: 277231.364180 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "overall_train_loss: 203839.5279052863\n",
      "overall_Test_loss: 277646.105576 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "overall_train_loss: 203711.9823404476\n",
      "overall_Test_loss: 277499.133638 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "overall_train_loss: 203804.7301563695\n",
      "overall_Test_loss: 276911.351496 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "overall_train_loss: 203650.94770513335\n",
      "overall_Test_loss: 277120.776981 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "overall_train_loss: 203621.57604186336\n",
      "overall_Test_loss: 277170.059071 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "overall_train_loss: 204193.14342423156\n",
      "overall_Test_loss: 276918.158074 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "overall_train_loss: 203595.76050959527\n",
      "overall_Test_loss: 277190.327419 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "overall_train_loss: 203566.53184853215\n",
      "overall_Test_loss: 277198.160276 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "overall_train_loss: 203660.10247049714\n",
      "overall_Test_loss: 277333.717375 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "overall_train_loss: 203654.07233148464\n",
      "overall_Test_loss: 277026.698708 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "overall_train_loss: 203553.91515596793\n",
      "overall_Test_loss: 277295.084557 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "overall_train_loss: 203667.8341496326\n",
      "overall_Test_loss: 277490.080475 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "overall_train_loss: 203796.94707770646\n",
      "overall_Test_loss: 276959.229494 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "overall_train_loss: 203545.47377429157\n",
      "overall_Test_loss: 277169.593292 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "overall_train_loss: 203671.1373650171\n",
      "overall_Test_loss: 277466.214680 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, batch_size, overall_train_loss):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        overall_train_loss += loss.item()\n",
    "        \n",
    "        #if batch % 2 == 0:\n",
    "        #   loss, current = loss.item(), (batch+1) * batch_size\n",
    "        #    print(f\"Train_loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    print('overall_train_loss: '+str(overall_train_loss))\n",
    "    return overall_train_loss\n",
    "\n",
    "def test(dataloader, model, loss_fn, overall_test_loss):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "    #test_loss /= num_batches\n",
    "    print(f\"overall_Test_loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss\n",
    "\n",
    "epochs = 200\n",
    "otrl = []\n",
    "otel = []\n",
    "for t in range(epochs):\n",
    "    overall_train_loss = 0\n",
    "    overall_test_loss = 0\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    otrl.append(train(train_dataloader, model, loss_fn, optimizer, batch_size, overall_train_loss))\n",
    "    otel.append(test(test_dataloader, model, loss_fn, overall_test_loss))\n",
    "    \n",
    "xc = np.arange(len(otrl))\n",
    "plt.plot(xc, otrl, '-b', label = 'Overall train loss')\n",
    "plt.plot(xc, otel, '-g', label = 'overall test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
